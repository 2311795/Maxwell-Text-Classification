{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Job Description Classification and Response Prediction\n",
    "\n",
    "This notebook demonstrates how to preprocess job descriptions, train machine learning models (Logistic Regression, SVM, Random Forest), and predict responses to job descriptions using different classifiers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in c:\\users\\nosao\\anaconda3\\lib\\site-packages (3.8.1)\n",
      "Requirement already satisfied: pandas in c:\\users\\nosao\\anaconda3\\lib\\site-packages (2.1.4)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\nosao\\anaconda3\\lib\\site-packages (1.2.2)\n",
      "Requirement already satisfied: click in c:\\users\\nosao\\anaconda3\\lib\\site-packages (from nltk) (8.1.7)\n",
      "Requirement already satisfied: joblib in c:\\users\\nosao\\anaconda3\\lib\\site-packages (from nltk) (1.2.0)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\nosao\\anaconda3\\lib\\site-packages (from nltk) (2023.10.3)\n",
      "Requirement already satisfied: tqdm in c:\\users\\nosao\\anaconda3\\lib\\site-packages (from nltk) (4.65.0)\n",
      "Requirement already satisfied: numpy<2,>=1.23.2 in c:\\users\\nosao\\anaconda3\\lib\\site-packages (from pandas) (1.26.4)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\nosao\\anaconda3\\lib\\site-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\nosao\\anaconda3\\lib\\site-packages (from pandas) (2023.3.post1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in c:\\users\\nosao\\anaconda3\\lib\\site-packages (from pandas) (2023.3)\n",
      "Requirement already satisfied: scipy>=1.3.2 in c:\\users\\nosao\\anaconda3\\lib\\site-packages (from scikit-learn) (1.11.4)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\nosao\\anaconda3\\lib\\site-packages (from scikit-learn) (2.2.0)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\nosao\\anaconda3\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\nosao\\anaconda3\\lib\\site-packages (from click->nltk) (0.4.6)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\nosao\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\nosao\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\nosao\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique labels in the dataset before mapping: ['Response C' 'Response B' 'Response A' 'Response D']\n",
      "Training Logistic Regression...\n",
      "Fitting 5 folds for each of 162 candidates, totalling 810 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\nosao\\anaconda3\\Lib\\site-packages\\sklearn\\model_selection\\_split.py:700: UserWarning: The least populated class in y has only 2 members, which is less than n_splits=5.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression best params: {'clf__C': 10, 'clf__solver': 'lbfgs', 'tfidf__max_df': 0.8, 'tfidf__min_df': 3, 'tfidf__ngram_range': (1, 2)}\n",
      "Logistic Regression best accuracy: 0.9385507246376813\n",
      "\n",
      "Logistic Regression classification report on test set:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.71      0.83      0.77        18\n",
      "           2       0.90      0.87      0.88        30\n",
      "           3       1.00      0.78      0.88         9\n",
      "\n",
      "    accuracy                           0.84        57\n",
      "   macro avg       0.87      0.83      0.84        57\n",
      "weighted avg       0.86      0.84      0.84        57\n",
      "\n",
      "Training SVM...\n",
      "Fitting 5 folds for each of 162 candidates, totalling 810 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\nosao\\anaconda3\\Lib\\site-packages\\sklearn\\model_selection\\_split.py:700: UserWarning: The least populated class in y has only 2 members, which is less than n_splits=5.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM best params: {'clf__C': 10, 'clf__kernel': 'linear', 'tfidf__max_df': 0.8, 'tfidf__min_df': 3, 'tfidf__ngram_range': (1, 1)}\n",
      "SVM best accuracy: 0.9211594202898551\n",
      "\n",
      "SVM classification report on test set:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.73      0.89      0.80        18\n",
      "           2       0.93      0.87      0.90        30\n",
      "           3       1.00      0.78      0.88         9\n",
      "\n",
      "    accuracy                           0.86        57\n",
      "   macro avg       0.89      0.84      0.86        57\n",
      "weighted avg       0.88      0.86      0.86        57\n",
      "\n",
      "Training Random Forest...\n",
      "Fitting 5 folds for each of 81 candidates, totalling 405 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\nosao\\anaconda3\\Lib\\site-packages\\sklearn\\model_selection\\_split.py:700: UserWarning: The least populated class in y has only 2 members, which is less than n_splits=5.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest best params: {'clf__n_estimators': 100, 'tfidf__max_df': 0.8, 'tfidf__min_df': 3, 'tfidf__ngram_range': (1, 2)}\n",
      "Random Forest best accuracy: 0.9208695652173912\n",
      "\n",
      "Random Forest classification report on test set:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.75      1.00      0.86        18\n",
      "           2       1.00      0.87      0.93        30\n",
      "           3       1.00      0.78      0.88         9\n",
      "\n",
      "    accuracy                           0.89        57\n",
      "   macro avg       0.92      0.88      0.89        57\n",
      "weighted avg       0.92      0.89      0.90        57\n",
      "\n",
      "\n",
      "Best model is Logistic Regression with accuracy of 0.9385507246376813\n",
      "Job Description: Manage university financial reports and budget forecasting.\n",
      "Predicted Response: Response C\n",
      "\n",
      "Job Description: Assist in organizing office files and managing schedules.\n",
      "Predicted Response: Response C\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "# pip is a package manager for Python libraries, and we use it to install the required libraries for this project.\n",
    "!pip install nltk pandas scikit-learn\n",
    "\n",
    "# Import essential modules and functions\n",
    "import pandas as pd  # For data manipulation and analysis\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV  # For splitting the data and performing grid search for hyperparameter tuning\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer  # For converting text data into TF-IDF features\n",
    "from sklearn.linear_model import LogisticRegression  # Logistic Regression classifier\n",
    "from sklearn.svm import SVC  # Support Vector Classifier\n",
    "from sklearn.ensemble import RandomForestClassifier  # Random Forest Classifier\n",
    "from sklearn.pipeline import Pipeline  # For creating machine learning pipelines\n",
    "from sklearn.metrics import classification_report, accuracy_score  # For evaluating model performance\n",
    "from nltk.corpus import stopwords  # For accessing a list of common stopwords\n",
    "from nltk.stem import WordNetLemmatizer  # For lemmatizing words (reducing words to their base form)\n",
    "from nltk.tokenize import word_tokenize  # For tokenizing text into words\n",
    "import nltk  # Natural Language Toolkit (NLTK) for text processing\n",
    "\n",
    "# Download necessary datasets from NLTK\n",
    "# These datasets include:\n",
    "# - 'punkt': used for tokenizing text into sentences and words\n",
    "# - 'stopwords': a list of common English stopwords to be removed during text processing\n",
    "# - 'wordnet': a lexical database for lemmatization\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "# Load the dataset into a Pandas DataFrame\n",
    "# The dataset a CSV file containing job descriptions and their corresponding labels.\n",
    "data = pd.read_csv('single_response.csv')\n",
    "\n",
    "# Preprocessing function to clean and prepare the text data\n",
    "def preprocess_text(text):\n",
    "    \"\"\"\n",
    "    Preprocesses a given text by:\n",
    "    - Converting the text to lowercase\n",
    "    - Tokenizing the text into words\n",
    "    - Lemmatizing each word (converting words to their base form)\n",
    "    - Removing stopwords and non-alphabetic tokens\n",
    "    - Returning the processed text as a single string\n",
    "    \n",
    "    Args:\n",
    "    text (str): The input text to preprocess.\n",
    "    \n",
    "    Returns:\n",
    "    str: The preprocessed text.\n",
    "    \"\"\"\n",
    "    lemmatizer = WordNetLemmatizer()  # Initialize the WordNet lemmatizer\n",
    "    tokens = word_tokenize(text.lower())  # Convert text to lowercase and tokenize\n",
    "    tokens = [lemmatizer.lemmatize(token) for token in tokens if token.isalpha() and token not in stopwords.words('english')]  # Lemmatize and remove stopwords\n",
    "    return ' '.join(tokens)  # Join tokens back into a single string\n",
    "\n",
    "# Apply the preprocessing function to the job descriptions\n",
    "# The column 'Job description' is expected to contain the raw text data.\n",
    "data['description'] = data['Job description'].apply(preprocess_text)\n",
    "\n",
    "# Print unique labels in the dataset to check for discrepancies\n",
    "# This step helps ensure that all labels are accounted for and correctly mapped.\n",
    "print(\"Unique labels in the dataset before mapping:\", data['Label'].unique())\n",
    "\n",
    "# Update the label mapping to match the labels in your dataset\n",
    "# This dictionary maps each label to a unique integer, which is required for model training.\n",
    "label_mapping = {\n",
    "    'Response A': 0,\n",
    "    'Response B': 1,\n",
    "    'Response C': 2,\n",
    "    'Response D': 3,\n",
    "    'Response E': 4,  # Adjust these mappings based on your dataset's labels\n",
    "    'Response F': 5   # Add or remove as necessary\n",
    "}\n",
    "\n",
    "# Apply the label mapping to the 'Label' column\n",
    "data['Label'] = data['Label'].map(label_mapping)\n",
    "\n",
    "# Check for any missing values in the 'Label' column after mapping\n",
    "if data['Label'].isnull().any():\n",
    "    # If there are missing values, print a message and the corresponding rows for further investigation.\n",
    "    print(\"There are missing values in the Label column. Please check the data.\")\n",
    "    print(data[data['Label'].isnull()])\n",
    "else:\n",
    "    # Split the data into training and testing sets\n",
    "    # X_train, X_test: Training and testing features (job descriptions)\n",
    "    # y_train, y_test: Training and testing labels\n",
    "    X_train, X_test, y_train, y_test = train_test_split(data['description'], data['Label'], test_size=0.2, random_state=42)\n",
    "\n",
    "    # Define pipelines for different classifiers\n",
    "    # A pipeline sequentially applies a list of transforms and a final estimator.\n",
    "    pipelines = {\n",
    "        'Logistic Regression': Pipeline([\n",
    "            ('tfidf', TfidfVectorizer()),  # Converts text data to TF-IDF features\n",
    "            ('clf', LogisticRegression(class_weight='balanced'))  # Logistic Regression classifier with balanced class weights\n",
    "        ]),\n",
    "        'SVM': Pipeline([\n",
    "            ('tfidf', TfidfVectorizer()),\n",
    "            ('clf', SVC(class_weight='balanced', probability=True))  # Support Vector Machine with probability estimates\n",
    "        ]),\n",
    "        'Random Forest': Pipeline([\n",
    "            ('tfidf', TfidfVectorizer()),\n",
    "            ('clf', RandomForestClassifier(class_weight='balanced'))  # Random Forest classifier\n",
    "        ])\n",
    "    }\n",
    "\n",
    "    # Define parameter grids for hyperparameter tuning using GridSearchCV\n",
    "    # These grids specify the hyperparameters to be tuned for each classifier.\n",
    "    param_grids = {\n",
    "        'Logistic Regression': {\n",
    "            'tfidf__max_df': [0.8, 0.9, 1.0],  # Maximum document frequency for terms\n",
    "            'tfidf__min_df': [1, 2, 3],  # Minimum document frequency for terms\n",
    "            'tfidf__ngram_range': [(1, 1), (1, 2), (2, 2)],  # N-gram ranges to consider\n",
    "            'clf__C': [0.1, 1, 10],  # Inverse regularization strength for Logistic Regression\n",
    "            'clf__solver': ['liblinear', 'lbfgs']  # Solvers to use for Logistic Regression\n",
    "        },\n",
    "        'SVM': {\n",
    "            'tfidf__max_df': [0.8, 0.9, 1.0],\n",
    "            'tfidf__min_df': [1, 2, 3],\n",
    "            'tfidf__ngram_range': [(1, 1), (1, 2), (2, 2)],\n",
    "            'clf__C': [0.1, 1, 10],  # Regularization parameter for SVM\n",
    "            'clf__kernel': ['linear', 'rbf']  # Kernel types to use in the algorithm\n",
    "        },\n",
    "        'Random Forest': {\n",
    "            'tfidf__max_df': [0.8, 0.9, 1.0],\n",
    "            'tfidf__min_df': [1, 2, 3],\n",
    "            'tfidf__ngram_range': [(1, 1), (1, 2), (2, 2)],\n",
    "            'clf__n_estimators': [50, 100, 200]  # Number of trees in the forest\n",
    "        }\n",
    "    }\n",
    "\n",
    "    # Initialize variables to keep track of the best model\n",
    "    best_models = {}\n",
    "    best_accuracy = 0\n",
    "    best_model_name = None\n",
    "    best_model = None\n",
    "\n",
    "    # Perform GridSearchCV for each model in the pipelines dictionary\n",
    "    for model_name in pipelines:\n",
    "        print(f\"Training {model_name}...\")\n",
    "        grid_search = GridSearchCV(pipelines[model_name], param_grids[model_name], cv=5, scoring='accuracy', n_jobs=-1, verbose=2)\n",
    "        grid_search.fit(X_train, y_train)  # Fit the model using the training data\n",
    "        best_models[model_name] = grid_search.best_estimator_  # Store the best estimator for each model\n",
    "        accuracy = grid_search.best_score_  # Retrieve the best accuracy score\n",
    "        print(f\"{model_name} best params: {grid_search.best_params_}\")\n",
    "        print(f\"{model_name} best accuracy: {accuracy}\")\n",
    "\n",
    "        # Evaluate each model on the test set\n",
    "        y_pred = best_models[model_name].predict(X_test)\n",
    "        print(f'\\n{model_name} classification report on test set:')\n",
    "        print(classification_report(y_test, y_pred))\n",
    "\n",
    "        # Check if this model has the highest accuracy so far\n",
    "        if accuracy > best_accuracy:\n",
    "            best_accuracy = accuracy\n",
    "            best_model_name = model_name\n",
    "            best_model = best_models[model_name]\n",
    "\n",
    "    # Output the best model and its accuracy\n",
    "    print(f\"\\nBest model is {best_model_name} with accuracy of {best_accuracy}\")\n",
    "\n",
    "    # Function to predict responses for new job descriptions using the best model\n",
    "    def predict_responses(job_descriptions):\n",
    "        \"\"\"\n",
    "        Predicts the response category for a list of job descriptions.\n",
    "        \n",
    "        Args:\n",
    "        job_descriptions (list): A list of job descriptions to classify.\n",
    "        \n",
    "        Returns:\n",
    "        list: Predicted response categories corresponding to the input job descriptions.\n",
    "        \"\"\"\n",
    "        processed_descriptions = [preprocess_text(desc) for desc in job_descriptions]  # Preprocess the job descriptions\n",
    "        predictions = best_model.predict(processed_descriptions)  # Predict using the best model\n",
    "        reverse_label_mapping = {v: k for k, v in label_mapping.items()}  # Reverse the label mapping for readable output\n",
    "        return [reverse_label_mapping[prediction] for prediction in predictions]  # Return human-readable predictions\n",
    "\n",
    "    # Example usage of the prediction function\n",
    "    job_descriptions = [\n",
    "        \"Manage university financial reports and budget forecasting.\",\n",
    "        \"Assist in organizing office files and managing schedules.\"\n",
    "    ]\n",
    "    predicted_responses = predict_responses(job_descriptions)\n",
    "    for job_desc, response in zip(job_descriptions, predicted_responses):\n",
    "        print(f'Job Description: {job_desc}\\nPredicted Response: {response}\\n')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Job Description: Manage university financial reports and budget forecasting.\n",
      "Predicted Response: Response C\n",
      "\n",
      "Job Description: Assist in organizing office files and managing schedules.\n",
      "Predicted Response: Response C\n",
      "\n",
      "Job Description: To plan, manage and deliver student recruitment events based on-campus and virtually..\n",
      "Predicted Response: Response C\n",
      "\n",
      "Job Description: \tTo monitor, review and report on the impact and effectiveness of all student recruitment events…\n",
      "Predicted Response: Response B\n",
      "\n",
      "Job Description: The Director of PMO & Strategic Change will lead the delivery of the University’s strategic change portfolio and Programme Management Offices. The Director will work in partnership with senior leaders and key stakeholders across the institution to ensure the delivery of the strategic change programme that underpins delivery of the University’s strategic plan.\n",
      "Predicted Response: Response A\n",
      "\n",
      "Job Description: To contribute to continuing improvements to management and financial systems and to the maintenance of effective administration.\n",
      "Predicted Response: Response D\n",
      "\n"
     ]
    }
   ],
   "source": [
    "    # Function to predict the response for multiple job descriptions using the best model\n",
    "def predict_responses(job_descriptions):\n",
    "  processed_descriptions = [preprocess_text(desc) for desc in job_descriptions]\n",
    "  predictions = best_model.predict(processed_descriptions)\n",
    "  reverse_label_mapping = {v: k for k, v in label_mapping.items()}\n",
    "  return [reverse_label_mapping[prediction] for prediction in predictions]  \n",
    "\n",
    "# Example usage\n",
    "job_descriptions = [\n",
    "\"Manage university financial reports and budget forecasting.\",\n",
    "\"Assist in organizing office files and managing schedules.\",\n",
    "\"To plan, manage and deliver student recruitment events based on-campus and virtually..\",\n",
    "\"\tTo monitor, review and report on the impact and effectiveness of all student recruitment events…\",\n",
    "\"The Director of PMO & Strategic Change will lead the delivery of the University’s strategic change portfolio and Programme Management Offices. The Director will work in partnership with senior leaders and key stakeholders across the institution to ensure the delivery of the strategic change programme that underpins delivery of the University’s strategic plan.\",\n",
    "\"To contribute to continuing improvements to management and financial systems and to the maintenance of effective administration.\"\n",
    "\n",
    "]\n",
    "predicted_responses = predict_responses(job_descriptions)\n",
    "for job_desc, response in zip(job_descriptions, predicted_responses):\n",
    "  print(f'Job Description: {job_desc}\\nPredicted Response: {response}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
